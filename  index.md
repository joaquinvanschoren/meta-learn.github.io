---
title: Home
permalink: /index.html
weight: -1
---

Recent years have seen rapid progress in meta-learning methods, which learn (and optimize) the performance of learning methods based on data, generate new learning methods from scratch, and learn to transfer knowledge across tasks and domains. Meta-learning can be seen as the logical conclusion of the arc that machine learning has undergone in the last decade, from learning classifiers, to learning representations, and finally to learning algorithms that themselves acquire representations and classifiers. The ability to improve one’s own learning capabilities through experience can also be viewed as a hallmark of intelligent beings, and there are strong connections with work on human learning in neuroscience.

Meta-learning methods are also of substantial practical interest, since they have, e.g., been shown to yield new state-of-the-art automated machine learning methods, novel deep learning architectures, and substantially improved one-shot learning systems. 

Some of the fundamental questions that this workshop aims to address are:
- What are the fundamental differences in the learning “task” compared to traditional  “non-meta” learners?
- Is there a practical limit to the number of meta-learning layers (e.g., would a meta-meta-meta-learning algorithm be of practical use)?
- How can we design more sample-efficient meta-learning methods?
- How can we exploit our domain knowledge to effectively guide the meta-learning process?
- What are the meta-learning processes in nature (e.g, in humans), and how can we take inspiration from them?
- Which ML approaches are best suited for meta-learning, in which circumstances, and why?
- What principles can we learn from meta-learning to help us design the next generation of learning systems? 

The goal of this workshop is to bring together researchers from all the different communities and topics that fall under the umbrella of meta-learning. We expect that the presence of these different communities will result in a fruitful exchange of ideas and stimulate an open discussion about the current challenges in meta-learning, as well as possible solutions.


## Speakers ##
- [Josh Tenenbaum](http://web.mit.edu/cocosci/josh.html) (MIT)
- [Jane Wang]() (DeepMind)
- [Jitendra Malik](https://people.eecs.berkeley.edu/~malik/) (UC Berkeley)
- [Oriol Vinyals]() (DeepMind)
- [Chelsea Finn](https://people.eecs.berkeley.edu/~cbfinn/) (UC Berkeley) 
- [Christophe Giraud-Carrier]() (Brigham Young University)

## Additional Panelists  ##
- [Samy Bengio]() (Google)

## Organizers ##
- [Roberto Calandra](http://www.robertocalandra.com) (UC Berkeley)
- [Frank Hutter](http://www2.informatik.uni-freiburg.de/~hutter/) (University of Freiburg)
- [Hugo Larochelle](http://www.dmi.usherb.ca/~larocheh/index_en.html) (Google Brain)
- [Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/) (UC Berkeley)

## Important dates ##
- Submission deadline: ~~03 November 2017~~ ([Anywhere on Earth](https://www.timeanddate.com/time/zones/aoe))
- Notification: ~~24 November 2017~~
- Camera ready: 04 December 2017
- Workshop: 09 December 2017

## Schedule ##

| --------:| ---------------------------------------------------
| 08:30 | Introduction and opening remarks 
| 08:40 | Invited talk 1: Jitendra Malik
| 09:10 | Invited talk 2: Christophe Giraud-Carrier -- Informing the Use of Hyperparameter Optimization Through Metalearning
| 09:40	| Poster spotlights
| 10:00 | Poster session 1 ( + Coffee Break)
| 11:00 | Invited talk 3: Jane Wang -- Multiple scales of task and reward-based learning
| 11:30 | Invited talk 4: Chelsea Finn
| 12:00 | *Lunch Break*
| 13:30 | Invited talk 5: Josh Tenenbaum
| 14:00 | Contributed talk 1: Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start
| 14:15 | Contributed talk 2: Learning to Model the Tail 
| 14:30 | Poster session 2 ( + Coffee Break)
| 15:30 | Invited talk 6: Oriol Vinyals
| 16:00 | Panel discussion
| 17:00 | End 


## Submission instructions  ##

Papers must be in the latest NIPS format, but with a maximum of 4 pages (excluding references). Papers should include the authors (by using the \nipsfinalcopy) to your document prior to submitting). 

Accepted papers and eventual supplementary material will be made available on the workshop website. However, this does not constitute an archival publication and no formal workshop proceedings will be made available, meaning contributors are free to publish their work in archival journals or conference.

*The two best papers submitted will be presented as 15-minutes contributed talks*

**Accepted authors will be able to register to the NIPS workshops even if the registration for public is currently closed!**

Submissions can be made at [https://cmt3.research.microsoft.com/metalearn2017](https://cmt3.research.microsoft.com/metalearn2017)

## Accepted Papers  ##

- SMASH: One-Shot Model Architecture Search through HyperNetworks
- Meta Inverse Reinforcement Learning via Maximum Reward Sharing
- Learning to Learn from Weak Supervision by Full Supervision
- Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm [[Extended version](https://arxiv.org/pdf/1710.11622)]
- Bayesian model ensembling using meta-trained recurrent neural networks
- Accelerating Neural Architecture Search using Performance Prediction
- Meta-Learning for Semi-Supervised Few-Shot Classification
- Connectivity Learning in Multi-Branch Networks
- A Simple Neural Attentive Meta-Learner
- Semi-Supervised Few-Shot Learning with Prototypical Networks
- Language Learning as Meta-Learning
- Hyperparameter Optimization with Hypernets
- Few-Shot Learning with Meta Metric Learners
- Gated Fast Weights for On-The-Fly Neural Program Generation
- A bridge between hyperparameter optimization \\ and learning-to-learn
- Understanding Short-Horizon Bias in Stochastic Meta-Optimization
- Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning
- Learning Decision Trees with Reinforcement Learning
- Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start
- Backpropagated plasticity: learning to learn with gradient descent in large plastic networks
- Learning to Learn while Learning
- Meta-Learning for Instance-Level Data Association
- Supervised Learning of Unsupervised Learning Rules
- Learning word embeddings from dictionary definitions only
- Learning to Model the Tail [[Extended version](https://papers.nips.cc/paper/7278-learning-to-model-the-tail)]
- Born Again Neural Networks
- Hyperactivations for Activation Function Exploration
- Concept Learning via Meta-Optimization with Energy Models
- Simple and Efficient Architecture Search for CNNs


## Program Committee ##

We thank the program committee for shaping the excellent technical program (in alphabetical order):   

Parminder Bhatia, Andrew Brock, Bistra Dilkina, Rocky Duan, David Duvenaud, Thomas Elsken, Dumitru Erhan, Matthias Feurer, Chelsea Finn, Roman Garnett, Christophe Giraud-Carrier, Erin Grant, Klaus Greff, Roger Grosse, Abhishek Gupta, Matt Hoffman, Aaron Klein, Marius Lindauer, Jan-Hendrik Metzen, Igor Mordatch, Randy Olson, Sachin Ravi, Horst Samulowitz, Jürgen Schmidhuber, Matthias	Seeger, Jake Snell, Jasper Snoek, Alexander	Toshev, Eleni Triantafillou, Jan van Rijn, Joaquin Vanschoren.

## Contacts  ##

For any question you can contact us at <info@metalearning.ml>

## Sponsors ##
<img src="https://upload.wikimedia.org/wikipedia/commons/2/2f/Google_2015_logo.svg" alt="googlelogo" title="google logo" height="100" />

 
